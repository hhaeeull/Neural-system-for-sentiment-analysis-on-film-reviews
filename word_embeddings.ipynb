{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8fKDWGQlT_hP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YowcYrkUOwG"
      },
      "source": [
        "Loading PyTorch\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2dazbs4RArm"
      },
      "source": [
        "# Imports PyTorch.\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEOouXSvWiNA"
      },
      "source": [
        "Downloading the dataset\n",
        "==\n",
        "The dataset we are going to use is the Large Movie Review Dataset (https://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD8AIWygRhI5"
      },
      "source": [
        "# Downloads the dataset.\n",
        "import urllib\n",
        "\n",
        "tmp = urllib.request.urlretrieve(\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "filename = tmp[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXNSehWtRsXE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e664091a-41bc-4287-e1b5-8b0086213cf3"
      },
      "source": [
        "filename"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/tmp/tmptf2y_u1p'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "097EOlPhS07G"
      },
      "source": [
        "# Extracts the dataset.\n",
        "import tarfile\n",
        "tar = tarfile.open(filename)\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp3XCqJ5TS73"
      },
      "source": [
        "import os # Useful library to read files and inspect directories."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOruARLhTU4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfadbe8-7562-4fca-a321-13ea499c5e0a"
      },
      "source": [
        "# Shows which files and directories are present at the root of the file system.\n",
        "for filename in os.listdir(\".\"):\n",
        "  print(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".config\n",
            "aclImdb\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqYr8dYnRtCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8d13c0-657f-44ec-f668-37b047ceab97"
      },
      "source": [
        "dataset_root = \"aclImdb\"\n",
        "# Shows which files and directories are present at the root of the dataset directory.\n",
        "for filename in os.listdir(dataset_root):\n",
        "  print(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n",
            "imdb.vocab\n",
            "README\n",
            "imdbEr.txt\n",
            "train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM9B2NreR-MT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc051bef-0f68-43d5-d634-383d053f3121"
      },
      "source": [
        "# Shows several reviews.\n",
        "dirname = os.path.join(dataset_root, \"train\", \"neg\") # \"aclImdb/{train|test}/{neg|pos}\"\n",
        "for idx, filename in enumerate(os.listdir(dirname)):\n",
        "  if(idx >= 5): break # Stops after the 5th file.\n",
        "\n",
        "  print(filename)\n",
        "  with open(os.path.join(dirname, filename)) as f:\n",
        "    review = f.read()\n",
        "    print(review)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "766_4.txt\n",
            "Whenever a Columbo story deviates from the familiar plot (colorful killer commits crime, Columbo smokes out killer, Columbo becomes a pest in the process), the writers somehow are never able to match the quality and interest of most traditional episodes. This episode deviates in the extreme, and the result is a major flop.<br /><br /> Would you believe: Columbo never faces the villain till the very end?!!<br /><br />Frankly, I was tempted to turn it off about two-thirds through.<br /><br /> Oh, the sacrifices we self-appointed reviewers make!!!\n",
            "\n",
            "6373_2.txt\n",
            "Oh this was a really bad movie. The girl who plays Jennifer is OK, but I think she acts bitchy through the movie, not because she is having her organs ripped out by a raven at night, but because she is thinking of firing her agent for putting her in this piece of crap. Faye Dunaway acts like she is remaking Mommy Dearest and the ending is completely silly. I really can't recommend this movie at all even though as a fan of Edgar Allen Poe, I was intrigued a bit by the references to his work(Ravens, House of Usher-like family curse,and being buried alive)-still he would probably turn over in his grave if he knew about this film.\n",
            "\n",
            "5375_2.txt\n",
            "Having just watched this movie, I almost feel like having wasted 2 hours of my life, but I guess there is some good in everything:<br /><br />If I was to rate this as any other movie, it can only receive 1 or 2 tops, but if I grade it like a low budget ind. movie, it may get 3 or 4. That is a movie is supposed to be 'complete' and without too long passages of boredom or waste of time. This movie isn't. But I guess a lot of independent movies are about showing movie skills, and considering this, this movie has a few highlights. If I am to comment on what the directors should take with them to their next project, I guess the distorted sound effects had some quality. They also manage to build some characters, this however takes me to what they should leave out in their next project, because the character building takes too long, since it is mostly irrelevant for the movie plot. Neither should the long spaces of time dedicated to walking around be continued in the next project - whats the point? I guess this movie tries to be a little bit of everything (building characters, suspense and a plot), and ends up being nothing (not a lot)<br /><br />This movie tries too much and too hard, and I guess it should have been cut to a short film. I could easily manage to find one hour of walking around or pointless dialogue to cut from the movie.<br /><br />There is too much irrelevant things going on in this movie. The story should have been more streamlined. I know there is supposed to be some mystery in this movie, but a slight surprise to who the killer is, doesn't make a mystery. The story behind the \"mystery\" receives almost no attention during the film, which leaves the final \"point\" as a quick an unsatisfying wrap-up. <br /><br />Therefore I would like to say this movie was a nice try, but I cant. I hope the directors learn from their mistakes, and produce a better product next time.<br /><br />If you don't have an interest in bench learning from producing low budget movies, there is no need to watch this - not even too see why everyone thinks its bad.<br /><br />As others have stated I am pretty sure the many 10's given to this movie are from people somehow involved in the movie. This movie could not receive a \"10\" judging from any remotely objective standpoint.\n",
            "\n",
            "6097_1.txt\n",
            "Back in 1985 I caught this thing (I can't even call it a movie) on cable. I was in college and I was with a high school friend whose hormones were raging out of control. I figured out early on that this was hopeless. Stupid script (a bunch of old guys hiring some young guys to show them how to score with women), bad acting (with one exception) and pathetic jokes. The plentiful female nudity here kept my friend happy for a while--but even he was bored after 30 minutes in. Remember--this was a HIGH SCHOOL BOY! This was back before nudity was so easy to get to by the Internet and such. We kept watching hoping for something interesting or funny but that never happened. The funniest thing about this was the original ad campaign in which the studio admitted this film was crap! (One poster had a fictional review that said, \"This is the best movie I've seen this afternoon!\"). Only Grant Cramer in the lead showed any talent and has actually gone on to a career in the business. No-budget and boring t&a. Skip it.\n",
            "\n",
            "12010_4.txt\n",
            "The acronymic \"F.P.1\" stands for \"Floating Platform #1\". The film portends the building of an \"F.P.1\" in the middle of the Atlantic Ocean, to be used as an \"air station\" for transatlantic plane flights. Based a contemporary Curt Siodmark novel; it was filmed in German as \"F.P.1 antwortet nicht\" (1932), in French as \"I.F.1 ne répond plus\" (1933), and in English as \"F.P.1\" (1933). Soon, technology made non-stop oceanic travel much more preferable.<br /><br />Stars Conrad Veidt (as Ellissen), Jill Esmond (as Droste), and Leslie Fenton (as Claire) find love and sabotage on and off the Atlantic platform. Karl Hartl directed. Mr. Veidt is most fun to watch; but, he is not convincing in the \"love triangle\" with Ms. Esmond and Mr. Fenton. The younger co-stars were the spouses of Laurence Olivier and Ann Dvorak, respectively. Both the concept and film have not aged well. <br /><br />**** F.P.1 (4/3/33) Karl Hartl ~ Conrad Veidt, Jill Esmond, Leslie Fenton\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikcb0jJiaotG"
      },
      "source": [
        "Preprocessing the dataset\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfsLxzRGctTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6aecaf-d8f9-4c9c-b954-2f70ecb99089"
      },
      "source": [
        "import nltk # Imports NLTK, an NLP library.\n",
        "nltk.download('punkt') # Loads a module required for tokenization.\n",
        "import collections # This library defines useful data structures."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NWWSBBYUoto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63da41db-d550-4f0f-e938-afb949867dc1"
      },
      "source": [
        "newline = \"<br />\" # The reviews sometimes contain this HTLM tag to indicate a line break.\n",
        "def preprocess(text):\n",
        "  text = text.replace(newline, \" \") # Replaces the newline HTML tag with a space.\n",
        "  tokens = nltk.word_tokenize(text); # Converts the text to a list of tokens (strings).\n",
        "  tokens = [token.lower() for token in tokens] # Lowercases all tokens.\n",
        "\n",
        "  return tokens\n",
        "\n",
        "# Reads and pre-processes the reviews.\n",
        "dataset = {\"train\": [], \"test\": []}\n",
        "binary_classes = {\"neg\": 0, \"pos\": 1}\n",
        "for part_name, l in dataset.items():\n",
        "  for class_name, value in binary_classes.items():\n",
        "    path = os.path.join(dataset_root, part_name, class_name)\n",
        "    print(\"Processing %s...\" % path, end='');\n",
        "    for filename in os.listdir(path):\n",
        "        with open(os.path.join(path, filename)) as f:\n",
        "          review_text = f.read()\n",
        "          review_tokens = preprocess(review_text)\n",
        "\n",
        "          l.append((review_tokens, value))\n",
        "    print(\" done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing aclImdb/train/neg... done\n",
            "Processing aclImdb/train/pos... done\n",
            "Processing aclImdb/test/neg... done\n",
            "Processing aclImdb/test/pos... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlDG9piYiVHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1596693f-be06-4431-f463-80ee11118099"
      },
      "source": [
        "# Splits the train set into a proper train set and a development/validation set.\n",
        "# 'dataset[\"train\"]' happens to be a list composed of a certain number of negative examples followed by the same number of positive examples.\n",
        "# We are going to use 3/4 of the original train set as our actual train set, and 1/4 as our development set.\n",
        "# We want to keep balanced train and development sets, i.e. for both, half of the reviews should be positive and half should be negative.\n",
        "if(\"dev\" in dataset): print(\"This should only be run once.\")\n",
        "else:\n",
        "  dev_set_half_size = int((len(dataset[\"train\"]) / 4) / 2) # Half of a quarter of the training set size.\n",
        "  dataset[\"dev\"] = dataset[\"train\"][:dev_set_half_size] + dataset[\"train\"][-dev_set_half_size:] # Takes some negative examples at the beginning and some positive ones at the end.\n",
        "  dataset[\"train\"] = dataset[\"train\"][dev_set_half_size:-dev_set_half_size] # Removes the examples used for the development set.\n",
        "\n",
        "  for (part, data) in dataset.items():\n",
        "    class_counts = collections.defaultdict(int)\n",
        "    for (_, p) in data: class_counts[p] += 1\n",
        "    print(f\"{part}: {class_counts}\")\n",
        "  print(\"Train set split into train/dev.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: defaultdict(<class 'int'>, {0: 9375, 1: 9375})\n",
            "test: defaultdict(<class 'int'>, {0: 12500, 1: 12500})\n",
            "dev: defaultdict(<class 'int'>, {0: 3125, 1: 3125})\n",
            "Train set split into train/dev.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfdibF5dhMIh"
      },
      "source": [
        "Loading the word embeddings\n",
        "==\n",
        "We are going to use GloVe embeddings.\n",
        "\n",
        "All word forms with a frequency below a given threshold are going to be considered unknown forms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnkKPGLYxQNR"
      },
      "source": [
        "# Computes the frequency of all word forms in the train set.\n",
        "word_counts = collections.defaultdict(int)\n",
        "for tokens, _ in dataset[\"train\"]:\n",
        "  for token in tokens: word_counts[token] += 1\n",
        "\n",
        "# print(word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgw19ofeZ23K"
      },
      "source": [
        "# Builds a vocabulary containing only those words present in the train set with a frequency above a given threshold.\n",
        "count_threshold = 4;\n",
        "vocabulary = set()\n",
        "for word, count in word_counts.items():\n",
        "    if(count > count_threshold): vocabulary.add(word)\n",
        "\n",
        "# print(vocabulary)\n",
        "# print(len(vocabulary))  #26317"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi6oZU1kph03"
      },
      "source": [
        "import zipfile\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnhHpcuxckbK"
      },
      "source": [
        "# Returns a dictionary {word[String]: id[Integer]} and a list of Numpy arrays.\n",
        "# `data_path` is the path of the directory containing the GloVe files (if None, 'glove.6B' is used)\n",
        "# `max_size` is the number of word embeddings read (starting from the most frequent; in the GloVe files, the words are sorted)\n",
        "# If `vocabulary` is specified (as a set of strings, or a dictionary from strings to integers), the output vocabulary contains the intersection of `vocabulary` and the words with a defined embedding. Otherwise, all words with a defined embedding are used.\n",
        "def get_glove(dim=50, vocabulary=None, max_size=-1, data_path=None):\n",
        "  dimensions = set([50, 100, 200, 300]) # Available dimensions for GloVe 6B\n",
        "  fallback_url = 'http://nlp.stanford.edu/data/glove.6B.zip' # (Remember that in GloVe 6B, words are lowercased.)\n",
        "\n",
        "  assert (dim in dimensions), (f'Unavailable GloVe 6B dimension: {dim}.')\n",
        "\n",
        "  if(data_path is None): data_path = 'glove.6B'\n",
        "\n",
        "  # Checks that the data is here, otherwise downloads it.\n",
        "  if(not os.path.isdir(data_path)):\n",
        "    #print('Directory \"%s\" does not exist. Creation.' % data_path)\n",
        "    os.makedirs(data_path)\n",
        "\n",
        "  glove_weights_file_path = os.path.join(data_path, f'glove.6B.{dim}d.txt')\n",
        "\n",
        "  if(not os.path.isfile(glove_weights_file_path)):\n",
        "    local_zip_file_path = os.path.join(data_path, os.path.basename(fallback_url))\n",
        "\n",
        "    if(not os.path.isfile(local_zip_file_path)):\n",
        "      print(f'Retreiving GloVe embeddings from {fallback_url}.')\n",
        "      urllib.request.urlretrieve(fallback_url, local_zip_file_path)\n",
        "\n",
        "    with zipfile.ZipFile(local_zip_file_path, 'r') as z:\n",
        "      print(f'Extracting GloVe embeddings from {local_zip_file_path}.')\n",
        "      z.extractall(path=data_path)\n",
        "\n",
        "  assert os.path.isfile(glove_weights_file_path), (f\"GloVe file {glove_weights_file_path} not found.\")\n",
        "\n",
        "  # Reads GloVe data.\n",
        "  print('Reading GloVe embeddings.')\n",
        "  new_vocabulary = {} # A dictionary {word[String]: id[Integer]}\n",
        "  embeddings = [] # The list of embeddings (Numpy arrays)\n",
        "  with open(glove_weights_file_path, 'r') as f:\n",
        "    for line in f: # Each line consist of the word followed by a space and all of the coefficients of the vector separated by a space.\n",
        "      values = line.split()\n",
        "\n",
        "      # Here, I'm trying to detect where on the line the word ends and where the vector begins. As in some version(s) of GloVe words can contain spaces, this is not entirely trivial.\n",
        "      vector_part = ' '.join(values[-dim:])\n",
        "      x = line.find(vector_part)\n",
        "      word = line[:(x - 1)]\n",
        "\n",
        "      if((vocabulary is not None) and (not word in vocabulary)): # If a vocabulary was specified and if the word is not it…\n",
        "        continue # …this word is skipped.\n",
        "\n",
        "      new_vocabulary[word] = len(new_vocabulary)\n",
        "      embedding = np.asarray(values[-dim:], dtype=np.float32)\n",
        "      embeddings.append(embedding)\n",
        "\n",
        "      if(len(new_vocabulary) == max_size): break\n",
        "  print('(GloVe embeddings loaded.)')\n",
        "  print()\n",
        "\n",
        "  return (new_vocabulary, embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY9nJNVlpZiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e9832b-6f09-478c-b0ca-80ad37db39d7"
      },
      "source": [
        "%%time\n",
        "(new_vocabulary, embeddings) = get_glove(dim=50, vocabulary=vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retreiving GloVe embeddings from http://nlp.stanford.edu/data/glove.6B.zip.\n",
            "Extracting GloVe embeddings from glove.6B/glove.6B.zip.\n",
            "Reading GloVe embeddings.\n",
            "(GloVe embeddings loaded.)\n",
            "\n",
            "CPU times: user 22.4 s, sys: 5.94 s, total: 28.3 s\n",
            "Wall time: 3min 7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwaov0tWpeFZ"
      },
      "source": [
        "# print(len(new_vocabulary)) # 25532\n",
        "# print(new_vocabulary) # Shows each word and its id."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIY-HdiPhgL4"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x07xfo-Gux_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb025d3-62ff-41e7-efd9-3c09ffc9f36a"
      },
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "class BatchGenerator:\n",
        "  def __init__(self, dataset, vocabulary):\n",
        "    self.dataset = dataset\n",
        "    for part in self.dataset.values(): # Shuffles the dataset so that positive and negative examples are mixed.\n",
        "      np.random.shuffle(part)\n",
        "\n",
        "    self.vocabulary = vocabulary # Dictonary {word[String]: id[Integer]}\n",
        "    self.unknown_word_id = len(vocabulary) # Id for unknown forms\n",
        "    self.padding_idx = len(vocabulary) + 1 # Not all reviews of a given batch will have the same length. We will \"pad\" shorter reviews with a special token id so that the batch can be represented by a matrix.\n",
        "\n",
        "  def length(self, data_type='train'):\n",
        "    return len(self.dataset[data_type])\n",
        "\n",
        "  # Returns a random batch.\n",
        "  # Batches are output as a triples (word_ids, polarity, texts).\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def get_batch(self, batch_size, data_type, subset=None):\n",
        "    data = self.dataset[data_type] # selects the relevant portion of the dataset.\n",
        "\n",
        "    max_i = len(data) if(subset is None) else min(subset, len(data))\n",
        "    instance_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some instance ids.\n",
        "\n",
        "    return self._ids_to_batch(data, instance_ids)\n",
        "\n",
        "  def _ids_to_batch(self, data, instance_ids):\n",
        "    word_ids = [] # Will be a list of lists of word ids (Integer)\n",
        "    polarity = [] # Will be a list of review polarities (Boolean)\n",
        "    texts = [] # Will be a list of lists of words (String)\n",
        "    for instance_id in instance_ids:\n",
        "      text, p = data[instance_id]\n",
        "\n",
        "      word_ids.append([self.vocabulary.get(w, self.unknown_word_id) for w in text])\n",
        "      polarity.append(p)\n",
        "      texts.append(text)\n",
        "\n",
        "    # Padding\n",
        "    self.pad(word_ids)\n",
        "\n",
        "    word_ids = torch.tensor(word_ids, dtype=torch.long) # Conversion to a tensor\n",
        "    polarity = torch.tensor(polarity, dtype=torch.bool) # Conversion to a tensor\n",
        "\n",
        "    return (word_ids, polarity, texts) # We don't really need `texts` but it might be useful to debug the system.\n",
        "\n",
        "  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n",
        "  # In place\n",
        "  def pad(self, word_ids):\n",
        "    max_length = max([len(s) for s in word_ids])\n",
        "    for s in word_ids: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "\n",
        "  # Returns a generator of batches for a full epoch.\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def all_batches(self, batch_size, data_type=\"train\", subset=None):\n",
        "    data = self.dataset[data_type]\n",
        "\n",
        "    max_i = len(data) if(subset is None) else min(subset, len(data))\n",
        "\n",
        "    # Loop that generates all full batches (batches of size 'batch_size').\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = np.arange(i, (i + batch_size))\n",
        "      yield self._ids_to_batch(data, instance_ids)\n",
        "      i += batch_size\n",
        "\n",
        "    # Possibly generates the last (not full) batch.\n",
        "    if(i < max_i):\n",
        "      instance_ids = np.arange(i, max_i)\n",
        "      yield self._ids_to_batch(data, instance_ids)\n",
        "\n",
        "  # Turns a list of arbitrary pre-processed texts into a batch.\n",
        "  # This function will be used to infer the polarity of a unannotated review.\n",
        "  def turn_into_batch(self, texts):\n",
        "    word_ids = [[self.vocabulary.get(w, self.unknown_word_id) for w in text] for text in texts]\n",
        "    self.pad(word_ids)\n",
        "    return torch.tensor(word_ids, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(dataset=dataset, vocabulary=new_vocabulary)\n",
        "print(batch_generator.length('train')) # Prints the number of instance in the train set."
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v35rEb8l0_Kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f4274f-1bfc-4141-d05f-b2b696b6d929"
      },
      "source": [
        "tmp = batch_generator.get_batch(3, data_type=\"train\")\n",
        "print(tmp[0]) # Prints the matrix of token ids. This matrix is what will be fed as input to the model (defined below).\n",
        "print(tmp[1]) # Prints the vector of polarities. This vector will be used to compute the loss when training the model.\n",
        "print(tmp[2]) # Prints the list of reviews."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   40,  5112,   571,  ..., 25533, 25533, 25533],\n",
            "        [   36, 19306,    13,  ...,    30,  1043,     2],\n",
            "        [ 1918,   797,    87,  ..., 25533, 25533, 25533]])\n",
            "tensor([ True, False, False])\n",
            "[['i', 'liked', 'nearly', 'all', 'the', 'movies', 'in', 'the', 'dirty', 'harry', 'series', 'with', 'the', 'exception', 'of', 'the', 'one', 'i', 'think', 'is', 'titled', '``', 'enforcer', \"''\", '.', '``', 'deadpool', \"''\", 'was', 'a', 'bit', 'weak', 'in', 'areas', 'too', ',', 'but', 'i', 'still', 'enjoyed', 'it', '.', 'this', 'one', 'is', 'one', 'of', 'my', 'favorites', 'of', 'the', 'series', ',', 'if', 'nothing', 'else', 'for', 'the', 'great', 'line', 'of', '``', 'go', 'ahead', ',', 'make', 'my', 'day', \"''\", '.', 'this', 'one', 'also', 'features', 'an', 'interesting', 'albeit', 'familiar', 'plot', 'of', 'someone', 'killing', 'those', 'that', 'have', 'done', 'her', 'wrong', '.', 'just', 'think', '``', 'magnum', 'force', \"''\", 'with', 'less', 'mystery', 'about', 'who', 'is', 'behind', 'the', 'killings', 'and', 'you', 'have', 'your', 'plot', '.', 'granted', 'there', 'is', 'a', 'bit', 'more', 'than', 'that', 'as', 'this', 'one', 'does', 'feature', 'a', 'very', 'nice', 'final', 'showdown', 'at', 'an', 'amusement', 'park', '.', 'it', 'also', 'features', 'dirty', 'harry', 'getting', 'a', 'bulldog', 'as', 'a', 'gift', 'and', 'it', 'tripping', 'up', 'sandra', 'locke', 'in', 'a', 'rather', 'humorous', 'scene', '.', 'the', 'only', 'question', 'that', 'remains', 'is', 'why', 'clint', 'eastwood', 'had', 'to', 'have', 'the', 'rather', 'mediocre', 'actress', 'sandra', 'locke', 'in', 'so', 'many', 'of', 'his', 'movies', '.', 'she', 'brings', 'the', 'score', 'down', 'a', 'point', 'every', 'time', 'even', 'when', 'overall', 'the', 'movie', 'is', 'enjoyable', 'to', 'me', '.', 'granted', 'she', 'is', 'not', 'to', 'bad', 'here', ',', 'but', 'her', 'character', 'could', 'have', 'been', 'so', 'much', 'better', 'by', 'someone', 'else', '.', 'another', 'problem', 'with', 'this', 'movie', 'and', 'other', 'dirty', 'harry', 'movies', ',', 'at', 'times', 'they', 'almost', 'seem', 'to', 'be', 'advertisements', 'for', 'guns', '.', 'i', 'like', 'guns', 'as', 'much', 'as', 'the', 'next', 'person', ',', 'but', 'do', 'we', 'really', 'need', 'scenes', 'of', 'him', 'explaining', 'all', 'the', 'different', 'strengths', 'of', 'his', 'newest', 'weapon', 'and', 'how', 'many', 'bullets', 'it', 'holds', '?', 'still', ',', 'very', 'nice', 'entry', 'into', 'the', 'dirty', 'harry', 'series', 'of', 'movies', '.'], ['this', 'reboot', 'is', 'like', 'a', 'processed', 'mcdonald', \"'s\", 'meal', 'compared', 'to', 'ang', 'lee', \"'s\", 'very', 'good', 'but', 'very', 'underrated', '2003', '``', 'the', 'incredible', 'hulk', \"''\", '.', 'ang', 'lee', \"'s\", '``', 'the', 'hulk', \"''\", 'is', 'a', 'comic', 'book', 'movie', 'for', 'the', 'thinking', 'person', '.', 'the', 'hulk', 'takes', 'some', 'time', 'to', 'appear', '(', 'about', '40', 'minutes', ')', ',', 'but', 'when', 'he', 'does', 'we', 'see', 'the', 'conflict', 'at', 'our', 'protagonist', \"'s\", 'core', '.', 'he', 'does', 'his', 'best', 'to', 'avoid', 'losing', 'control', ',', 'but', 'as', 'he', 'admits', ',', 'when', 'he', 'does', 'give', 'in', 'to', 'his', 'rage', ',', 'he', 'likes', 'it', '.', 'now', 'compare', 'this', 'to', 'edward', 'norton', \"'s\", 'turn', ',', 'where', 'there', 'is', 'no', 'display', 'of', 'conflict', 'in', 'his', 'personality', ',', 'and', 'he', 'turns', 'into', 'the', 'hulk', 'whenever', 'he', 'becomes', 'excited', 'and', 'his', 'pulse', 'rate', 'hits', '200', 'beats', 'per', 'minute', '(', 'not', '183', ',', 'not', '197', ',', 'but', 'exactly', '200', ')', '.', 'to', 'this', 'reviewer', ',', 'this', 'felt', 'akin', 'to', 'the', 'introduction', 'of', 'midi-chlorians', 'in', 'the', 'phantom', 'menace', ',', 'which', 'tell', 'how', 'strong', 'the', '``', 'force', \"''\", 'is', 'with', 'one', ',', 'as', 'if', 'mystical', 'abilities', 'can', 'be', 'gauged', 'through', 'a', 'blood', 'test', '.', 'in', 'the', '2008', 'movie', ',', 'all', 'ed', 'norton', \"'s\", 'bruce', 'banner', 'has', 'to', 'do', 'is', 'to', 'keep', 'his', 'pulse', 'rate', 'under', '200', ',', 'as', 'monitored', 'by', 'a', 'device', 'strapped', 'onto', 'his', 'wrist', '.', 'and', 'for', 'the', 'record', ',', 'it', 'is', 'extremely', 'difficult', 'to', 'get', 'one', \"'s\", 'pulse', 'rate', 'up', 'to', '200', 'even', 'through', 'a', 'very', 'exhilarating', 'run', ',', 'especially', 'for', 'a', 'physically', 'fit', 'person', '.', 'emotions', 'drive', 'eric', 'bana', \"'s\", 'hulk', '.', 'he', 'has', 'repressed', 'memories', 'of', 'his', 'mother', \"'s\", 'death', 'and', 'his', 'father', \"'s\", 'role', 'in', 'his', 'early', 'life', '.', 'on', 'the', 'surface', 'he', 'is', 'calm', ',', 'but', 'there', 'underneath', 'there', 'is', 'significant', 'anger', ',', 'enough', 'rage', 'to', 'fuel', 'the', 'hulk', 'when', 'it', 'is', 'unleashed', '.', 'but', 'the', 'hulk', 'never', 'kills', 'intentionally', ',', 'even', 'his', 'attackers', '.', 'mostly', ',', 'he', 'just', 'wants', 'to', 'get', 'away', 'or', 'close', 'to', 'betty', 'ross', '(', 'jennifer', 'connelly', ')', '.', 'he', 'incapacitates', 'his', 'shooters', 'and', 'in', 'one', 'critical', 'scene', ',', 'saves', 'a', 'fighter', 'jet', 'pursuing', 'him', 'from', 'a', 'collision', '.', 'in', 'the', '2003', 'movie', ',', 'the', 'visual', 'effects', 'are', 'there', ',', 'but', 'as', 'necessary', '.', 'the', 'action', 'scenes', '(', 'and', 'there', 'are', 'plenty', 'of', 'them', ')', 'are', 'necessary', 'to', 'the', 'plot', ',', 'unlike', 'the', '2008', 'movie', 'where', 'action', 'is', 'inserted', 'for', 'the', 'sake', 'of', 'action', '.', 'in', 'his', 'quest', 'to', 'turn', 'it', 'into', 'a', '``', 'fugitive', \"''\", 'story', ',', 'ed', 'norton', 'loses', 'focus', 'on', 'how', 'bruce', 'banner', 'feels', 'about', 'the', 'hulk', '.', 'to', 'him', ',', 'the', 'hulk', 'is', 'a', 'dangerous', 'side', 'effect', 'of', 'an', 'experiment', 'went', 'awry', '.', 'and', 'louis', 'leterrier', ',', 'the', 'director', 'picking', 'up', 'on', 'ed', 'norton', \"'s\", 'cue', ',', 'fails', 'to', 'add', 'any', 'emotional', 'dimension', 'to', 'the', 'banner-hulk', 'relationship', '.', 'the', 'story', 'can', 'be', 'summed', 'up', 'as', ':', 'banner', 'is', 'pursued', 'and', 'hulk', 'kicks', 'butt', ',', 'banner', 'is', 'pursed', 'and', 'hulk', 'kick', 'butt', '.', 'repeat', 'as', 'necessary', '.', 'the', 'only', 'thing', 'that', 'went', 'against', 'the', '2003', 'movie', 'is', 'that', 'it', 'is', 'perhaps', 'not', 'as', 'true', 'to', 'the', 'comic', 'books', 'as', 'the', '2008', 'version', '.', 'however', ',', 'even', 'as', 'i', 'am', 'a', 'fan', 'of', 'the', 'comics', ',', 'i', 'would', 'prefer', 'a', 'movie', 'that', 'is', 'not', 'as', 'true', 'to', 'the', 'comics', 'as', 'a', 'fanboy', \"'s\", 'dream', 'but', 'has', 'heart', 'over', 'the', 'comics-loyal', 'but', 'manufactured', 'and', 'soulless', '2008', 'version', 'any', 'day', '.', 'perhaps', 'ang', 'lee', 'should', 'have', 'read', 'more', 'comics', '.', 'iron', 'man', ',', 'also', 'released', 'in', '2008', ',', 'is', 'a', 'perfect', 'example', 'of', 'a', 'movie', 'that', 'is', 'true', 'to', 'its', 'comic-book', 'roots', 'and', 'has', 'heart', '.'], ['warning', '!', 'do', \"n't\", 'even', 'consider', 'watching', 'this', 'film', 'in', 'any', 'form', '.', 'it', \"'s\", 'not', 'even', 'worth', 'downloading', 'from', 'the', 'internet', '.', 'every', 'bit', 'of', 'porn', 'has', 'more', 'substance', 'than', 'this', 'wasted', 'piece', 'of', 'celluloid', '.', 'the', 'so-called', 'filmmakers', 'apparently', 'have', 'absolutely', 'no', 'idea', 'how', 'to', 'make', 'a', 'film', '.', 'they', 'could', \"n't\", 'tell', 'a', 'good', 'joke', 'to', 'save', 'their', 'lives', '.', 'it', \"'s\", 'an', 'insult', 'to', 'any', 'human', 'being', '.', 'if', 'you', \"'re\", 'looking', 'for', 'a', 'fun-filled', 'movie', '-', 'go', 'look', 'somewhere', 'else', '.', 'let', \"'s\", 'hope', 'this', 'mr.', 'unterwaldt', '(', 'the', '``', 'jr.', \"''\", 'being', 'a', 'good', 'indication', 'for', 'his', 'obvious', 'inexperience', 'and', 'intellectual', 'infancy', ')', 'dies', 'a', 'slow/painful', 'death', 'and', 'never', 'makes', 'a', 'film', 'again', '.', 'in', 'fact', ',', 'it', \"'s\", 'even', 'a', 'waste', 'of', 'time', 'to', 'write', 'anything', 'about', 'this', 'crap', ',', 'that', \"'s\", 'why', 'i', \"'ll\", 'stop', 'right', 'now', 'and', 'rather', 'watch', 'a', 'good', 'film', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp2VOTBzFLb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa2c3a58-b0e1-48c1-c936-620584a897d3"
      },
      "source": [
        "len(list(batch_generator.all_batches(batch_size=3, data_type=\"train\"))) # Number of batches in the training set for batches of size 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6250"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsTuIZoIhkTW"
      },
      "source": [
        "The model\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE6mbJ9Q2nUy"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SentimentClassifier(torch.nn.Module):\n",
        "    def __init__(self, embeddings, hidden_sizes, freeze_embeddings=True, device='cpu'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create padding token vector (zeros) and unknown token vector (average of all embeddings)\n",
        "        padding_vector = torch.zeros(embeddings[0].shape)\n",
        "        unknown_vector = torch.mean(torch.stack([torch.tensor(e) for e in embeddings]), dim=0)\n",
        "\n",
        "        # Append unknown and padding vectors to the embeddings list\n",
        "        embeddings.append(unknown_vector.numpy())\n",
        "        embeddings.append(padding_vector.numpy())\n",
        "\n",
        "        # Convert to tensor and create the embedding layer\n",
        "        embedding_matrix = torch.tensor(embeddings)\n",
        "        self.padding_idx = len(embeddings) - 1  # Padding index is the last element\n",
        "        self.unknown_idx = len(embeddings) - 2  # Unknown token index is second to last\n",
        "\n",
        "        self.embeddings = torch.nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings, padding_idx=self.padding_idx)\n",
        "        self.embeddings = self.embeddings.to(device)  # Send to device\n",
        "\n",
        "        # Define the main part of the network (sequence of linear layers)\n",
        "        layers = []\n",
        "        input_size = embedding_matrix.shape[1]  # Input size is the embedding dimension\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(torch.nn.Linear(input_size, hidden_size))\n",
        "            layers.append(torch.nn.ReLU())\n",
        "            input_size = hidden_size\n",
        "\n",
        "        # Output layer (final linear layer to predict sentiment score)\n",
        "        layers.append(torch.nn.Linear(input_size, 1))\n",
        "\n",
        "        self.main_part = torch.nn.Sequential(*layers)\n",
        "        self.main_part = self.main_part.to(device)  # Send to device\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Turn batch into embeddings\n",
        "        embeds = self.embeddings(batch)  # batch shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Create mask to ignore padding embeddings\n",
        "        mask = (batch != self.padding_idx).unsqueeze(-1).float()  # shape: (batch_size, seq_length, 1)\n",
        "\n",
        "        # Compute the average of the embeddings (ignoring padding tokens)\n",
        "        sum_embeddings = torch.sum(embeds * mask, dim=1)\n",
        "        sum_mask = torch.sum(mask, dim=1)  # Count how many non-padding tokens per review\n",
        "\n",
        "        avg_embeddings = sum_embeddings / sum_mask  # Avoids dividing by zero\n",
        "\n",
        "        # Pass the averaged embeddings through the network\n",
        "        output = self.main_part(avg_embeddings).squeeze(1)  # Squeeze to make the shape (batch_size,)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpqcxScW4Afb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8173f6-0f9a-42a4-c2d2-4a9a93d5fefc"
      },
      "source": [
        "model = SentimentClassifier(embeddings, hidden_sizes=[100], freeze_embeddings=True)\n",
        "batch = batch_generator.get_batch(3, data_type=\"train\")\n",
        "print(model(batch[0])) # This output (its shape) should be checked."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0635, -0.0522, -0.0715], grad_fn=<SqueezeBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-bd331b998fa5>:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  embedding_matrix = torch.tensor(embeddings)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmL7aPacmNVh"
      },
      "source": [
        "# Function that computes the accuracy of the model on a given part of the dataset.\n",
        "evaluation_batch_size = 256\n",
        "def evaluation(model, data_type, subset=None):\n",
        "  nb_correct = 0\n",
        "  total = 0\n",
        "  for batch in batch_generator.all_batches(evaluation_batch_size, data_type=data_type, subset=subset):\n",
        "    prob = model(batch[0].to(model.device)) # Forward pass\n",
        "    answer = (prob > 0.5) # Shape: (evaluation_batch_size, 1)\n",
        "    nb_correct += (answer == batch[1].to(model.device)).sum().item()\n",
        "    total += batch[0].shape[0]\n",
        "\n",
        "  accuracy = (nb_correct / total)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_gHaFzrOaj"
      },
      "source": [
        "Training\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOMCyVPK6BD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35087717-fc59-4a56-89bf-6d3cc86cd24e"
      },
      "source": [
        "# Initialize variables before the training loop\n",
        "nb_epoch = 20  # Number of epochs\n",
        "epoch_id = 0   # Id of the current epoch\n",
        "instances_processed = 0  # Number of instances trained on in the current epoch\n",
        "epoch_loss = []  # Will contain the loss for each batch of the current epoch\n",
        "batch_size = 16 # Batch size\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "epoch_size = len(list(batch_generator.all_batches(batch_size, data_type=\"train\")))  # Number of batches in the training set\n",
        "\n",
        "while epoch_id < nb_epoch:\n",
        "    model.train()  # Switch to training mode\n",
        "\n",
        "    model.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Get a batch of training data (you can ignore 'texts' using the underscore '_')\n",
        "    batch, labels, _ = batch_generator.get_batch(batch_size, data_type=\"train\", subset=None)\n",
        "    batch, labels = batch.to(model.device), labels.to(model.device)  # Move batch and labels to the device\n",
        "\n",
        "\n",
        "    # (i) Compute model predictions\n",
        "    predictions = model(batch)\n",
        "\n",
        "    # (ii) Compute the loss\n",
        "    loss_function = torch.nn.BCEWithLogitsLoss()  # Binary classification loss function\n",
        "    loss = loss_function(predictions, labels.float())  # Assuming labels are binary (0 or 1)\n",
        "\n",
        "    # (iii) Backpropagate the loss\n",
        "    loss.backward()\n",
        "\n",
        "    # (iv) Store the loss\n",
        "    epoch_loss.append(loss.item())\n",
        "\n",
        "    # Update model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track the number of instances processed\n",
        "    instances_processed += batch_size\n",
        "\n",
        "    # End of epoch: Print statistics and evaluate the model\n",
        "    if instances_processed >= epoch_size:\n",
        "        print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "        print(f\"Average loss: {sum(epoch_loss) / len(epoch_loss)}.\")\n",
        "\n",
        "        # Evaluation: Switch to evaluation mode\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Compute accuracy on training set\n",
        "            train_accuracy = evaluation(model, \"train\")\n",
        "            print(f\"Accuracy on the train set: {train_accuracy}.\")\n",
        "\n",
        "            # Compute accuracy on validation set (dev set)\n",
        "            dev_accuracy = evaluation(model, \"dev\")\n",
        "            print(f\"Accuracy on the dev set: {dev_accuracy}.\")\n",
        "\n",
        "        # Increment epoch counter and reset the loss for the next epoch\n",
        "        epoch_id += 1\n",
        "        instances_processed -= epoch_size\n",
        "        epoch_loss = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- END OF EPOCH 0.\n",
            "Average loss: 0.6867781909736427.\n",
            "Accuracy on the train set: 0.5.\n",
            "Accuracy on the dev set: 0.5.\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 0.6732716478713571.\n",
            "Accuracy on the train set: 0.52368.\n",
            "Accuracy on the dev set: 0.52528.\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 0.6564809949430701.\n",
            "Accuracy on the train set: 0.5415466666666666.\n",
            "Accuracy on the dev set: 0.54448.\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 0.6407832601299025.\n",
            "Accuracy on the train set: 0.5788266666666667.\n",
            "Accuracy on the dev set: 0.57888.\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 0.6160501716910182.\n",
            "Accuracy on the train set: 0.65568.\n",
            "Accuracy on the dev set: 0.65456.\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 0.6075445585054894.\n",
            "Accuracy on the train set: 0.62064.\n",
            "Accuracy on the dev set: 0.62448.\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 0.5917632363430442.\n",
            "Accuracy on the train set: 0.64544.\n",
            "Accuracy on the dev set: 0.64736.\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 0.5751003202510206.\n",
            "Accuracy on the train set: 0.7.\n",
            "Accuracy on the dev set: 0.7016.\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 0.5804547264769271.\n",
            "Accuracy on the train set: 0.6157333333333334.\n",
            "Accuracy on the dev set: 0.6208.\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 0.5608212315056422.\n",
            "Accuracy on the train set: 0.61552.\n",
            "Accuracy on the dev set: 0.6224.\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 0.5738350928646244.\n",
            "Accuracy on the train set: 0.66112.\n",
            "Accuracy on the dev set: 0.6624.\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 0.5669339197139217.\n",
            "Accuracy on the train set: 0.7240533333333333.\n",
            "Accuracy on the dev set: 0.72224.\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 0.5844443943854924.\n",
            "Accuracy on the train set: 0.71136.\n",
            "Accuracy on the dev set: 0.7152.\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 0.5607012383741875.\n",
            "Accuracy on the train set: 0.7127466666666666.\n",
            "Accuracy on the dev set: 0.71632.\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 0.5346191860225102.\n",
            "Accuracy on the train set: 0.6745066666666667.\n",
            "Accuracy on the dev set: 0.67344.\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 0.5503571123293002.\n",
            "Accuracy on the train set: 0.6909333333333333.\n",
            "Accuracy on the dev set: 0.68848.\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 0.5547475822874017.\n",
            "Accuracy on the train set: 0.7389866666666667.\n",
            "Accuracy on the dev set: 0.74048.\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 0.5329496770688932.\n",
            "Accuracy on the train set: 0.7434133333333334.\n",
            "Accuracy on the dev set: 0.74256.\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 0.5280777722188871.\n",
            "Accuracy on the train set: 0.74496.\n",
            "Accuracy on the dev set: 0.74624.\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 0.5234992451047245.\n",
            "Accuracy on the train set: 0.6928533333333333.\n",
            "Accuracy on the dev set: 0.69664.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh4na1hivgXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c31dd3-a0b6-44ad-c58f-b1c172fe0d53"
      },
      "source": [
        "model.eval() # Tells PyTorch that we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\", \"Bad.\", \"Not bad!\"]]).to(model.device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-3.4477,  4.1759, -7.2726, -7.3947], grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}